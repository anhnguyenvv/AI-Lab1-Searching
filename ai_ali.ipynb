{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anhnguyenvv/AI-Lab1-Searching/blob/main/ai_ali.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjLr_-jz7r9W",
        "outputId": "98990c3f-bc66-4ceb-bcdf-9dc3061ace6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.3/316.3 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,129 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,377 kB]\n",
            "Get:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,858 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [47.6 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,084 kB]\n",
            "Fetched 6,907 kB in 2s (3,281 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "51 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 51 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio -q\n",
        "!pip install openai -q\n",
        "!pip install gtts -q\n",
        "!pip install pydub -q\n",
        "!pip install config -q\n",
        "!pip install git+https://github.com/openai/whisper.git -q\n",
        "!sudo apt update && sudo apt install ffmpeg\n",
        "!pip install gdown -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gTENxSh492It"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import config\n",
        "from gtts import gTTS\n",
        "import os\n",
        "import subprocess\n",
        "from pydub import AudioSegment\n",
        "import math\n",
        "import whisper\n",
        "from openai import OpenAI\n",
        "import requests, gdown\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNtL67nSBNSq"
      },
      "source": [
        "### load whisper model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAlxhz_lFejg",
        "outputId": "7f1d2763-6f49-4d49-dbac-e37e63e579fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 139M/139M [00:06<00:00, 22.9MiB/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Checking if NVIDIA GPU is available\n",
        "torch.cuda.is_available()\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "W_model = whisper.load_model(\"base\", device=DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F81WeS25R7fC",
        "outputId": "8eeaa444-8670-4b42-9eb2-a2f2098584dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1jpM-ZP0h6OXDyuAfN1V6U7fvkdZuLLpU\n",
            "To: /content/Samples/Sample1.mp3\n",
            "100%|██████████| 7.69M/7.69M [00:00<00:00, 26.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1uwpA8Wuix2QJ0gvvnZk7CtaSsoC21iEj\n",
            "To: /content/Samples/Sample2.mp3\n",
            "100%|██████████| 8.13M/8.13M [00:00<00:00, 23.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1iLKcK5Z9IaYrKzaPQLMIZm3vJ7DssCAz\n",
            "To: /content/Samples/Sample3.mp3\n",
            "100%|██████████| 10.1M/10.1M [00:00<00:00, 27.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "#File Project1_Data.json\n",
        "file_id = [\"1jpM-ZP0h6OXDyuAfN1V6U7fvkdZuLLpU\",\n",
        "           \"1uwpA8Wuix2QJ0gvvnZk7CtaSsoC21iEj\",\n",
        "           \"1iLKcK5Z9IaYrKzaPQLMIZm3vJ7DssCAz\"\n",
        "]\n",
        "import os\n",
        "os.makedirs('Samples')\n",
        "datafilename =[ \"./Samples/Sample1.mp3\",\n",
        "        \"./Samples/Sample2.mp3\",\n",
        "        \"./Samples/Sample3.mp3\"]\n",
        "for i in range(len(file_id)):\n",
        "   with open(datafilename[i], 'wb') as f:\n",
        "     gdown.download(f\"https://drive.google.com/uc?id={file_id[i]}\", datafilename[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6F6riawW2JRw"
      },
      "outputs": [],
      "source": [
        "messages_system= {\"role\": \"system\",\n",
        "                        \"content\": '''You are a simulated IELTS Speaking examiner. Your primary task is to conduct a mock IELTS Speaking test, adhering closely to the IELTS standards.\n",
        "# the IELTS standards\n",
        "Part 1: Introduction and Interview (4-5 minutes)\n",
        "Begin by introducing yourself and confirming the candidate's identity.\n",
        "Ask the candidate to answer questions about familiar topics such as their home, family, work, studies, and interests. Example questions:\n",
        "Can you tell me about your hometown?\n",
        "What do you enjoy doing in your free time?\n",
        "Can you describe your job or what you are studying?\n",
        "Part 2: Long Turn (3-4 minutes)\n",
        "Provide the candidate with a task card that includes a topic and points to cover. Give the candidate 1 minute to prepare and make notes if they wish.\n",
        "Ask the candidate to speak for 1-2 minutes on the topic. Example task card:\n",
        "  Describe a memorable trip you have taken.\n",
        "  Where you went?\n",
        "  Why you went there?\n",
        "  What you did there?\n",
        "  Explain why this trip was memorable for you?\n",
        "  After the candidate's long turn, ask one or two follow-up questions related to the topic.\n",
        "Part 3: Discussion (4-5 minutes)\n",
        "Engage the candidate in a discussion based on the topic from Part 2. Ask more abstract and complex questions to allow the candidate to express opinions and ideas. Example questions:\n",
        "How do you think travel experiences can change a person's view of the world?\n",
        "What are the benefits and drawbacks of traveling to different countries?\n",
        "How has tourism impacted the environment in popular travel destinations?\n",
        "\n",
        "# do\n",
        "step 1, you ask user want to Simulator a mock IELTS Speaking test or analyze a transcript of existing recording speaking test.\n",
        "  if user given a transcript of IELTS speaking test audio of conversation betwwen examiner and test taker. you will classify the dialog into examiner and test taker, remember summary this dialog in your reponse. then, do next step.\n",
        "  else user want start a simulated or something. you start a mock IELTS Speaking as a examiner and scrictly comply with the IELTS standards and the regulations below:\n",
        "  - Once the script is executed, it will display the randomly selected topics for Part 1, and a single topic for Part 2 or 3.\n",
        "  - Limit the repetition of example questions in describing IELTS standards\n",
        "  - Pose only one question at a time, proceeding to the next on the same topic only after the student's response.\n",
        "  - Focus on asking questions and responding appropriately, without changing the student's words.\n",
        "\n",
        "step 2: After the student answers the final question. Provide constructive feedback based on their performance, highlighting strengths and areas for improvement. Provide a strict estimate of specific overall band scores and sub-scores(e.g. if the sub-score is 6.0-6.5, your estimation should be 6.0, which could help students) for fluency and coherence, lexical resource, grammatical range and accuracy, and pronunciation.\n",
        "Conclude the conversation and point out all mistake sentences from the conversation and offer constructive feedback and improvement strategies. Finally, include a disclaimer that this evaluation is generated by ChatGPT and is for reference only; official IELTS exam results may vary.\n",
        "'''\n",
        "                       }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLMjBhPzBTF7"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "-aMUTzWsOuDG",
        "outputId": "65172a67-2875-4f99-b986-8e2ce3950026"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://61ab2adf6007bb4eb1.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://61ab2adf6007bb4eb1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Set your OpenAI API key\n",
        "client = OpenAI(api_key = \"sk-oGx8ZhLZ0xMahXqAF3tGcsTQhfO7WGIiRC0UlwtNYKJU9pxu\",\n",
        "                base_url=\"https://api.chatanywhere.tech/v1\")\n",
        "#mess = \"\"\n",
        "#with open('prompt.txt', 'r') as file:\n",
        " ##   mess = file.read()\n",
        "#messages_system= {\"role\": \"system\",\n",
        "                 #      \"content\": mess\n",
        "                   #    }\n",
        "\n",
        "# Transcribe audio using OpenAI's Whisper\n",
        "def transcribe_audio(audio):\n",
        "    segment_length = 600000\n",
        "    # Open the audio file\n",
        "    audio_file = AudioSegment.from_file(audio)\n",
        "    # Get the duration of the audio file in milliseconds\n",
        "    duration_ms = len(audio_file)\n",
        "    # Calculate the number of segments needed\n",
        "    num_segments = math.ceil(duration_ms / segment_length)\n",
        "    # Create an empty string to hold the concatenated text\n",
        "    all_text = \"\"\n",
        "    # Split the audio file into segments\n",
        "    for i in range(num_segments):\n",
        "        start = i * segment_length\n",
        "        end = min((i + 1) * segment_length, duration_ms)\n",
        "        segment = audio_file[start:end]\n",
        "        segment.export(f\"segment_{i}.mp3\", format=\"mp3\")\n",
        "\n",
        "    for i in range(num_segments):\n",
        "        audio_file = open(f\"segment_{i}.mp3\", \"rb\")\n",
        "        transcript = W_model.transcribe(f\"segment_{i}.mp3\")\n",
        "        all_text += transcript['text']\n",
        "    return all_text\n",
        "import csv\n",
        "def respond(\n",
        "    message,\n",
        "    history: list[tuple[str, str]],\n",
        "    mic=None,\n",
        "    text = \"Simulator IELTS Speaking test\",\n",
        "):\n",
        "    global messages_system\n",
        "    result = ''\n",
        "    messages = [messages_system]\n",
        "    if len(history) != 0:\n",
        "      history_chat = []\n",
        "      with open('chat_transcript.csv', 'r') as f:\n",
        "        history_chat = csv.reader(f)\n",
        "        for val in history_chat:\n",
        "          if val[0]:\n",
        "              messages.append({\"role\": val[0], \"content\": val[1]})\n",
        "\n",
        "    if mic is not None:\n",
        "          audio = mic\n",
        "    elif  len(message[\"files\"])!= 0:\n",
        "          audio = message[\"files\"][0]\n",
        "          #return message[\"files\"][0]\n",
        "    else:\n",
        "          return \"You must either provide a mic recording or a file\"\n",
        "\n",
        "    if audio:\n",
        "        transcript = transcribe_audio(audio)\n",
        "    transcript += \"\\n\\n\" + message[\"text\"]\n",
        "\n",
        "    messages.append({\"role\": \"user\", \"content\": transcript})\n",
        "\n",
        "    response = \"\"\n",
        "    response = client.chat.completions.create(\n",
        "          messages= messages,\n",
        "          model=\"gpt-3.5-turbo\",\n",
        "      )\n",
        "    systems_message = response.choices[0].message.content\n",
        "    messages.append({\"role\": \"assistant\", \"content\": systems_message})\n",
        "    chat_transcript =[]\n",
        "    for message in messages:\n",
        "        if message['role'] != 'system':\n",
        "            chat_transcript.append([message['role'] , message['content']])\n",
        "    with open('chat_transcript.csv', 'w') as f:\n",
        "      writer = csv.writer(f)\n",
        "      writer.writerows(chat_transcript)\n",
        "    return systems_message +\"\\n\\n\"+ result\n",
        "\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "demo = gr.ChatInterface(\n",
        "    respond,\n",
        "    additional_inputs=[\n",
        "        gr.Audio(sources=\"microphone\", type=\"numpy\", label=\"Speak here...\"),\n",
        "         gr.Radio([\"Simulator IELTS Speaking test\",\n",
        "                                    \"Analyze of an existing IELTS Speaking record\" ,\n",
        "                                   ], label=\"\"),\n",
        "    ],\n",
        "    title=\"AI Auditor\",\n",
        "    description=\"AI Alliance for Audio Analytics Team. Our project's objective is to conduct a mock IELTS Speaking test, adhering closely to the IELTS standards.\",\n",
        "    textbox = gr.MultimodalTextbox(interactive=True, file_types=[\"audio\"], placeholder=\"upload file...\"),\n",
        "\n",
        "    multimodal=True,\n",
        ")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(debug = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vpx0mTSfBJuC"
      },
      "source": [
        "Demo by gradio Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eBJcmbmItWqw"
      },
      "outputs": [],
      "source": [
        "# Set your OpenAI API key\n",
        "client = OpenAI(api_key = \"sk-oGx8ZhLZ0xMahXqAF3tGcsTQhfO7WGIiRC0UlwtNYKJU9pxu\",\n",
        "                base_url=\"https://api.chatanywhere.tech/v1\")\n",
        "\n",
        "\n",
        "# Transcribe audio using OpenAI's Whisper\n",
        "def transcribe_audio(audio):\n",
        "    segment_length = 600000\n",
        "    # Open the audio file\n",
        "    audio_file = AudioSegment.from_file(audio)\n",
        "    # Get the duration of the audio file in milliseconds\n",
        "    duration_ms = len(audio_file)\n",
        "    # Calculate the number of segments needed\n",
        "    num_segments = math.ceil(duration_ms / segment_length)\n",
        "    # Create an empty string to hold the concatenated text\n",
        "    all_text = \"\"\n",
        "    # Split the audio file into segments\n",
        "    for i in range(num_segments):\n",
        "        start = i * segment_length\n",
        "        end = min((i + 1) * segment_length, duration_ms)\n",
        "        segment = audio_file[start:end]\n",
        "        segment.export(f\"segment_{i}.mp3\", format=\"mp3\")\n",
        "\n",
        "\n",
        "    for i in range(num_segments):\n",
        "        audio_file = open(f\"segment_{i}.mp3\", \"rb\")\n",
        "        transcript = W_model.transcribe(f\"segment_{i}.mp3\")\n",
        "        all_text += transcript['text']\n",
        "\n",
        "    return all_text\n",
        "\n",
        "# Analyze transcript using OpenAI's GPT-3\n",
        "def analyze_transcript(transcript):\n",
        "    global messages_system\n",
        "    messages=[]\n",
        "    messages.append(messages_system)\n",
        "    messages.append({\"role\": \"user\", \"content\": transcript})\n",
        "    response = client.chat.completions.create(\n",
        "          messages= messages,\n",
        "          model=\"gpt-3.5-turbo\",\n",
        "      )\n",
        "\n",
        "    systems_message = response.choices[0].message.content\n",
        "    messages.append({\"role\": \"assistant\", \"content\": systems_message})\n",
        "\n",
        "    chat_transcript = \"\"\n",
        "    for message in messages:\n",
        "        if message['role'] != 'system':\n",
        "            chat_transcript += message['role'] + \": \" + message['content'] + \"\\n\\n\"\n",
        "    result = \"Transcript: \" + transcript + \"\\n\\n\"\n",
        "    return systems_message #+\"\\n\\n\"+ result\n",
        "# Define the transcription and analysis function\n",
        "def transcribe_and_analyze(mic=None, file=None):\n",
        "    if mic is not None:\n",
        "        audio = mic\n",
        "    elif file is not None:\n",
        "        audio = file\n",
        "    else:\n",
        "        return \"You must either provide a mic recording or a file\"\n",
        "    # Transcribe the audio to text\n",
        "    transcript = transcribe_audio(audio)\n",
        "    # Analyze sentiment and summarize\n",
        "    analysis = analyze_transcript(transcript)\n",
        "    return analysis\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mXwSF6rDsAG1"
      },
      "outputs": [],
      "source": [
        "\n",
        "io = gr.Interface(fn=transcribe_and_analyze,\n",
        "                      inputs=[\n",
        "                          gr.Audio(sources=\"microphone\", type=\"numpy\", label=\"Speak here...\"),\n",
        "                          gr.Audio(sources=\"upload\", type = \"filepath\"),\n",
        "                      ],\n",
        "                      outputs=\"text\",\n",
        "                      title=\"AI Auditor for\",\n",
        "                      description='''AI Alliance for Audio Analytics Team. Our project's objective is to conduct a mock IELTS Speaking test, adhering closely to the IELTS standards\n",
        "                      ''',\n",
        "                      examples=[[None, \"./Samples/Sample1.mp3\"],\n",
        "                                [None, \"./Samples/Sample2.mp3\"],\n",
        "                                [None, \"./Samples/Sample3.mp3\"]],\n",
        "                      )\n",
        "\n",
        "io.launch(share=True, debug = True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}